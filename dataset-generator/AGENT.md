# AGENT GUIDE (dataset-generator)

You are an LLM agent helping maintain a pipeline that trains a Gemma 3 270M student (LoRA via Unsloth) to produce JSON ASTs that satisfy given JSON Schemas. Follow these practices to be useful and safe.

## Repo map (what matters)
- `config/defaults.json`: single source of truth for dataset/training/alignment defaults. Most CLIs accept `--config` to load these.
- `scripts/*.py`: all pipeline steps (dataset generation → SFT training → eval → DPO alignment → reporting).
- `run*.sh`: thin runners that call the scripts with sane defaults (`runAll.sh` orchestrates everything).
- `outputs/`: generated artifacts (d_* JSONL files, student checkpoints, eval results, reports). Treat as build outputs; do not hand-edit.
- `unsloth_compiled_cache/`: autogenerated; avoid editing unless explicitly patching around upstream bugs.

## How to work
1) **Set up env**: use Python 3.12+; install deps with `uv sync`. Ollama must be available locally for teacher steps (model resolved via `LLM_MODEL` or `.llmrc`).
2) **Read defaults first**: adjust `config/defaults.json` or override via CLI flags instead of hardcoding.
3) **Run commands** (examples; use `--help` for more):
   - Dataset: `./runDatasetGeneration.sh --config config/defaults.json`.
   - Training: `./runTraining.sh --config config/defaults.json [extra trainer flags]`.
   - Eval: `./runEvals.sh --adapter outputs/student_runs/gemma3-270m/checkpoint-final --teacher-model gpt-oss:120b`.
   - Alignment: `./runAlignment.sh --config config/defaults.json --adapter <checkpoint> --teacher-model <ollama_id>`.
4) **Keep changes minimal and reproducible**: prefer flag/config wiring over inlining constants; document new defaults in README if behavior shifts.

## Coding conventions
- Python target: 3.12. Stick to ASCII in code/docs. Add brief comments only when non-obvious.
- Avoid touching generated caches (`unsloth_compiled_cache`) unless intentionally hot-patching; if you must, keep patches small and explain why.
- Follow existing CLI patterns: argparse, defaults pulled from `config/defaults.json`, kebab-case flags.
- When editing training code, preserve filtering/sanity checks that guard against NaNs and truncated labels.

## Validation expectations
- Lightweight checks: `uv run python -m compileall scripts` or `uv run python -m py_compile <file>`.
- Targeted runs: use small caps (`--max-samples`, `--max-steps`, or lower seq length) to smoke-test training/eval changes.
- For reporting changes, ensure `outputs/student_runs/<run>/training_metrics.jsonl` and eval artifacts remain consumable by `scripts/report_training.py`.

## Output hygiene
- Do not commit large artifacts under `outputs/` unless explicitly requested. Generated files should be reproducible via the scripts/runners.
- Record new behaviors in README or inline docstrings so future agents know how to re-run or debug.

## When stuck
- Re-check `config/defaults.json` assumptions, the runner scripts, and `README.md` for expected behaviors.
- If an upstream Unsloth/TRL quirk appears, prefer a minimal local patch with a comment referencing the symptom.
